{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a989ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id_in:  0\n",
      "job:  0\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n",
      "len(features):  774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pdb\n",
    "import csv\n",
    "import io\n",
    "import numpy as np\n",
    "from statistics import stdev\n",
    "import json\n",
    "import orjson\n",
    "import math\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "\n",
    "all_compare_names = ['sameEv-sameSchema_sameEv-otherSchema',\n",
    "                        'sameEv-sameSchema_otherEv-sameSchema',\n",
    "                        'sameEv-sameSchema_otherEv-otherSchema',\n",
    "                        'sameEv-otherSchema_otherEv-sameSchema',\n",
    "                        'sameEv-otherSchema_otherEv-otherSchema',\n",
    "                        'otherEv-sameSchema_otherEv-otherSchema']\n",
    "\n",
    "# mark where each event starts and ends\n",
    "event_1_start = 17\n",
    "event_2_start = 23\n",
    "event_3_start = 34\n",
    "event_4_start = 50\n",
    "event_5_start = 66\n",
    "\n",
    "def run_jobs(input_dir, json_path, testing = False,  \n",
    "            output_dir = \"/scratch/network/rk1593/\", \n",
    "            only_events2_3_and_4 = True,\n",
    "            job_id_target = None, num_chunks = 31):\n",
    "    \"\"\"\n",
    "    run a job which is a a particular set of searchlights that we want\n",
    "    to turn into a tvalue vector for the purpose of clustering\n",
    "    \"\"\"\n",
    "    # open the jobs info dict which is a dictionary containing a mapping\n",
    "    # from job id (i.e. a number from 0 to 200) to the list of searchlights\n",
    "    # to process in that job\n",
    "    f = open(json_path,)\n",
    "    jobs_info_dict = json.load(f)\n",
    "    for job_id in range(jobs_info_dict[\"num_jobs_actual\"]):\n",
    "        # this is how we only process this one job of interest\n",
    "        if job_id != job_id_target:\n",
    "            continue\n",
    "        print(\"job: \", job_id)\n",
    "        this_job_searchlights = jobs_info_dict[\"job_id_to_searchlight_subset\"][str(job_id)][:10]\n",
    "        chunk = this_job_searchlights\n",
    "        go_from_480searchlight_files_representing_fingerprintPlot_to_tvalue_vector(\n",
    "                            chunk, \n",
    "                            testing, \n",
    "                            output_dir, \n",
    "                            only_events2_3_and_4,\n",
    "                            input_dir,\n",
    "                            )\n",
    "\n",
    "# requires: searchlight_to_files_tuples for all searchlights, subset_list_of_searchlights are a list of the searchlights we want to include for this job\n",
    "# outputs a csv file for each searchlight\n",
    "def go_from_480searchlight_files_representing_fingerprintPlot_to_tvalue_vector(subset_list_of_searchlights, \n",
    "            testing = False, \n",
    "            output_dir = \"\", \n",
    "             only_events2_3_and_4 = True,\n",
    "              in_dir = \"\"):\n",
    "    os.chdir(in_dir)\n",
    "    for counter_light, light_id in enumerate(subset_list_of_searchlights):\n",
    "        tar_file_name = light_id + \"_new.tar.gz\"\n",
    "        # check that we have a tar file for this searchlight\n",
    "        if not os.path.exists(in_dir + tar_file_name):\n",
    "            print(\"__________Light not on della yet!_________\")\n",
    "            continue\n",
    "        tar = tarfile.open(tar_file_name)\n",
    "        # get the 480 files and make sure we got the right number\n",
    "        files_this_light = [x.name for x in tar.getmembers()]\n",
    "        num_files_this_light = len(files_this_light)\n",
    "        if num_files_this_light != 480:\n",
    "            print(\"Error: searchlight \" + light_id + \" has \" + str(num_files_this_light) + \" files.\" )\n",
    "            np.savetxt(output_dir +  \"/searchlights_tval_example/\" + light_id + \"_NOT480.csv\", np.array([1,2,3]), delimiter=\",\")\n",
    "            return\n",
    "        # don't reprocess a searchlight again if we already did create\n",
    "        # tvalue vector for it in the past\n",
    "        save_path = output_dir +  \"/searchlights_tval_example/\" + light_id + \".csv\"\n",
    "        if os.path.exists(save_path):\n",
    "            print(\"Path Exists, Do Not Reprocess\")\n",
    "            continue\n",
    "        template2_count = 0\n",
    "        template3_count = 0\n",
    "        template4_count = 0\n",
    "        template_to_pid_to_cond_to_matrices = {} # create this dict for later usage when creating fingerprint plots\n",
    "        template_to_pid_to_cond_to_lists = {}\n",
    "        i = 0\n",
    "        # step 1 is to get dict mapping the template (2,3 or 4) to pid (N = 40) to cond (condition) where condition is synonymous\n",
    "        # with \"other-Event-same-Schema\" and so on\n",
    "        for file_name in files_this_light:\n",
    "            splitted_file_name = file_name.split(\"_\")\n",
    "            # get the participant id\n",
    "            pid = splitted_file_name[1]\n",
    "            cond = splitted_file_name[3] + \"-\" + splitted_file_name[4]\n",
    "            template_id = int(file_name[-5])\n",
    "            if file_name[-5] == \"2\":\n",
    "                template2_count += 1 \n",
    "            elif file_name[-5] == \"3\":\n",
    "                template3_count += 1 \n",
    "            elif file_name[-5] == \"4\":\n",
    "                template4_count += 1 \n",
    "            else:\n",
    "                print(\"Error: file_name template retrieval error.\")\n",
    "                print(file_name)\n",
    "                print(tar_file_name)\n",
    "                return\n",
    "            # change the directory to this in_dir to be able to open the tar file\n",
    "            os.chdir(in_dir)\n",
    "            i += 1\n",
    "            in_text = tar.extractfile(file_name).read()\n",
    "            csv_file = io.StringIO(in_text.decode('ascii'))\n",
    "            random_replacer_for_nothing = str(3e200) # this is set to be something super big (greater than 1), so that \n",
    "            # when we replace all numbers outside [-1,1] with NaN then these lines where no data is recorded become NaN\n",
    "            # cleaning_mean_start = time.time()\n",
    "            csv_lines = [[y.replace(\" \", \"\") for y in x] for x in csv.reader(csv_file)]\n",
    "            if testing:\n",
    "                csv_lines[0][0] = \"-1000\"\n",
    "            # replace any spots where there is no data collected with the random replacer (more explained above why I did this)\n",
    "            csv_lines = [[y if y != \"\" else random_replacer_for_nothing for y in x] for x in csv_lines]\n",
    "            try:\n",
    "                new_arr = np.array(csv_lines).astype(\"float\")\n",
    "                # replace the empty due to the random replacer and other anomaly large numbers outside of [-1,1] (the range for correlation) found in raw data\n",
    "                new_arr[abs(new_arr) > 1] = np.nan\n",
    "                # now we edit the new_arr so that 0-indexed row 9, columns 47 through 73 become nan for subject 102\n",
    "                if pid == \"sub-102\": # sub-102 had some issues in this area of the FMRI\n",
    "                    new_arr[9,47:74] = np.nan\n",
    "            except ValueError:\n",
    "                pdb.set_trace()\n",
    "            # if in testing mode we need to crop out the first column and the first row\n",
    "            # since these files were different \n",
    "            if testing:\n",
    "                mean_list = np.nanmean(new_arr[1:13, 1:75], axis = 0).tolist()\n",
    "                new_arr = new_arr[1:13, 1:75].tolist()\n",
    "            else:\n",
    "                mean_list = np.nanmean(new_arr, axis = 0).tolist()\n",
    "                new_arr = new_arr.tolist()\n",
    "            # error check if the mean list has an nan in it which means that all weddings in one tr had nan\n",
    "            if np.count_nonzero(np.isnan(mean_list)) != 0:\n",
    "                print(\"Error: mean list has np.nan\")\n",
    "                # if isnan has all Falses and so everything is 0\n",
    "                return \n",
    "            if str(template_id) not in template_to_pid_to_cond_to_matrices:\n",
    "                template_to_pid_to_cond_to_matrices[str(template_id)] = {}\n",
    "                template_to_pid_to_cond_to_lists[template_id] = {}\n",
    "            if pid not in template_to_pid_to_cond_to_matrices[str(template_id)]:\n",
    "                template_to_pid_to_cond_to_matrices[str(template_id)][pid] = {}\n",
    "                template_to_pid_to_cond_to_lists[template_id][pid] = {}\n",
    "            template_to_pid_to_cond_to_matrices[str(template_id)][pid][cond] = new_arr\n",
    "            template_to_pid_to_cond_to_lists[template_id][pid][cond] = mean_list\n",
    "        with open(output_dir + \"searchlights_matrices_orjson_example/\" + light_id, \"wb\") as f:\n",
    "            f.write(orjson.dumps(template_to_pid_to_cond_to_matrices))\n",
    "        # check that we got even number for each template for error check\n",
    "        if template2_count != 160 or template3_count != 160 or template4_count != 160:\n",
    "            print(\"Error!\")\n",
    "            print(\"tar_file_name: \", tar_file_name)\n",
    "            print(\"template2_count: \", template2_count)\n",
    "            print(\"template3_count: \", template3_count)\n",
    "            print(\"template4_count: \", template4_count)\n",
    "            return\n",
    "\n",
    "        # step 2 is to get the dict mapping the template to the comparison\n",
    "        # where there are 6 differen comparisons between the 4 conditions, to pid to \n",
    "        # a list of differences between one condition and another in the comparison\n",
    "        template_to_compare_to_pid_to_tr_diffs = {}\n",
    "        for template_id in template_to_pid_to_cond_to_lists:\n",
    "            if template_id not in template_to_compare_to_pid_to_tr_diffs:\n",
    "                template_to_compare_to_pid_to_tr_diffs[template_id] = {}\n",
    "            for pid in template_to_pid_to_cond_to_lists[template_id]:\n",
    "                for comparison_name in all_compare_names:\n",
    "                    compare1_name,compare2_name = comparison_name.split(\"_\")    \n",
    "                    if comparison_name not in template_to_compare_to_pid_to_tr_diffs[template_id]:\n",
    "                        template_to_compare_to_pid_to_tr_diffs[template_id][comparison_name] = {}\n",
    "                    compare1_list =  template_to_pid_to_cond_to_lists[template_id][pid][compare1_name]\n",
    "                    compare2_list = template_to_pid_to_cond_to_lists[template_id][pid][compare2_name]\n",
    "                    if len(compare1_list) != len(compare2_list):\n",
    "                        print(\"Error: for the same template and light, two paths have different number of tr's\")\n",
    "                        return\n",
    "                    tr_differences = [(compare1_list[i] - compare2_list[i]) for i in range(len(compare1_list))]\n",
    "                    if only_events2_3_and_4:\n",
    "                        tr_differences = tr_differences[event_2_start:event_5_start]\n",
    "                    template_to_compare_to_pid_to_tr_diffs[template_id][comparison_name][pid] = tr_differences\n",
    "\n",
    "        # step 3: is to get the across subject tvalue for each comparison\n",
    "        # at each TR for each template and comparison\n",
    "        template_to_compare_to_trTstats = {}\n",
    "        for template_id in template_to_compare_to_pid_to_tr_diffs:\n",
    "            for compare_name in template_to_compare_to_pid_to_tr_diffs[template_id]:\n",
    "                # check that all participants have the same length of tr_diffs\n",
    "                # while also getting list of diffs at each tr\n",
    "                length_tr_list = []\n",
    "                tr_num_to_list_of_diffs = {}\n",
    "                for pid in template_to_compare_to_pid_to_tr_diffs[template_id][compare_name]:\n",
    "                    tr_diffs = template_to_compare_to_pid_to_tr_diffs[template_id][compare_name][pid]\n",
    "                    for index,diff in enumerate(tr_diffs):\n",
    "                        if index not in tr_num_to_list_of_diffs:\n",
    "                            tr_num_to_list_of_diffs[index] = []\n",
    "                        tr_num_to_list_of_diffs[index].append(diff)\n",
    "                    length_tr_list.append(len(tr_diffs))\n",
    "                length_tr_list = np.array(length_tr_list)\n",
    "                if not np.all(length_tr_list[0] == length_tr_list):\n",
    "                    pdb.set_trace()\n",
    "                    print(\"Error: not all the same length_tr_lists\")\n",
    "                    return\n",
    "                # now go through each tr index, and get a ttest\n",
    "                # make sure that we have 40 participants in each tr num\n",
    "                # and make sure we don't have nan\n",
    "                for tr_num in tr_num_to_list_of_diffs:\n",
    "                    if len(tr_num_to_list_of_diffs[tr_num]) != 40 or np.isnan(sum(tr_num_to_list_of_diffs[tr_num])):\n",
    "                        print(tr_num_to_list_of_diffs[tr_num])\n",
    "                        pdb.set_trace()\n",
    "                        print(\"Error: missing participant or nan\")\n",
    "                        return\n",
    "               \n",
    "                tr_Tstats = [get_t_stat_of_list(tr_num_to_list_of_diffs[tr_num]) for tr_num in range(0,len(tr_num_to_list_of_diffs.keys()))]\n",
    "                if template_id not in template_to_compare_to_trTstats:\n",
    "                    template_to_compare_to_trTstats[template_id] = {}\n",
    "                template_to_compare_to_trTstats[template_id][compare_name] = tr_Tstats\n",
    "                # for each searchlight get a feature list\n",
    "        # step 4: compile those tstats into one vector which is our final\n",
    "        # tvalue vector representation that we will cluster\n",
    "        features = []\n",
    "        for template_id in [2,3,4]:\n",
    "            for compare_name in all_compare_names:\n",
    "                for tr_stat in template_to_compare_to_trTstats[template_id][compare_name]:\n",
    "                    features.append(tr_stat)\n",
    "        print(\"len(features): \", len(features))\n",
    "        # output it!\n",
    "        features_np = np.array(features)\n",
    "        np.savetxt(save_path, features_np, delimiter=\",\")\n",
    "\n",
    "def divide_chunks(l, chunk_size):\n",
    "    \"\"\"\n",
    "    take in a list and chunk size and cut this list up into chunks\n",
    "    \"\"\"\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), chunk_size): \n",
    "        yield l[i:i + chunk_size]\n",
    "\n",
    "def get_t_stat_of_list(list):\n",
    "    \"\"\"\n",
    "    take in a list of size 40 for each particpant for a particular tr and comparison\n",
    "    and output the tvalue\n",
    "    \"\"\"\n",
    "    n = len(list)\n",
    "    mean = (sum(list) / n)\n",
    "    sd = stdev(list)\n",
    "    sem = (sd / math.sqrt(n))\n",
    "    t_stat = (mean / sem)\n",
    "    return t_stat\n",
    "\n",
    "# DRIVER #\n",
    "bash_it = True\n",
    "input_dir = \"/scratch/gpfs/rk1593/tar_by_searchlight/tar_by_searchlight/\" # here we have stored a list of 480 files in a tar file for each searchlight\n",
    "output_dir = \"/scratch/gpfs/rk1593/clustering_output/\"  # output dhere on della\n",
    "json_file_name = \"jobs_info_dict_manual_jupyter_without_tuples.json\"\n",
    "testing = False\n",
    "num_chunks = 31\n",
    "only_events2_3_and_4 = True # we only want to look at event 2, 3 and 4\n",
    "job_id_in = 0\n",
    "print(\"job_id_in: \", job_id_in)\n",
    "if bash_it:\n",
    "    run_jobs(input_dir, json_path = output_dir + json_file_name, \n",
    "                testing = testing, \n",
    "            output_dir = output_dir,  only_events2_3_and_4 = only_events2_3_and_4,\n",
    "            job_id_target= job_id_in,\n",
    "            num_chunks= num_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad16927",
   "metadata": {},
   "source": [
    "# K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ac163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "extracted for /searchlights/ directory\n",
      "K_clusters:  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def kmeans_clustering(dir, K, X, searchlight_list):\n",
    "    # read in the master_X_list and master_searchlight so that we have the cluster assignments\n",
    "    # for the searchlights without having to output so much data each time\n",
    "    # by outputting the master_X_list to the output csv multiple times\n",
    "    # which would be a waste of della's memory\n",
    "    print(\"K_clusters: \", K)\n",
    "    kmeans = KMeans(n_clusters=K)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    # add 1 so that we have no label with zero\n",
    "    cluster_labels += 1\n",
    "    # create csv with the cluster assignment for this number of clusters\n",
    "    # create the output df by giving cluster labels\n",
    "    this_cost = kmeans.inertia_\n",
    "    cost_list = [this_cost for i in range(len(searchlight_list))]\n",
    "    output_dict = {\"cluster_assignment\": cluster_labels.tolist(),\n",
    "                \"searchlight\": searchlight_list,\n",
    "                \"cost\":cost_list\n",
    "                }\n",
    "    df = pd.DataFrame(output_dict)\n",
    "    df.to_csv(dir + \"kmeans_\" + str(K) + \"clusters_tval.csv\")\n",
    "    # It is important to use binary access\n",
    "    with open(dir + \"kmeans_\" + str(K) + \"clusters_tval.pickle\", 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "        \n",
    "        import numpy as np\n",
    "import os\n",
    "\n",
    "dir = \"/scratch/gpfs/rk1593/clustering_output/\"\n",
    "X_list = []\n",
    "searchlight_list = []\n",
    "for index,file in enumerate(os.scandir(dir + \"searchlights_tval_example/\")):\n",
    "    file_name = file.name\n",
    "    if \".csv\" in file_name:\n",
    "        if \"NOT480\" in file_name:\n",
    "            print(\"Error: NOT480 \", file_name)\n",
    "        if index % 10000 == 0:\n",
    "            print(index)\n",
    "        light_id = file_name.split(\".\")[0]\n",
    "        features = np.genfromtxt(dir + \"searchlights_tval_example/\" + file_name, delimiter = \",\").tolist()\n",
    "        searchlight_list += [light_id]\n",
    "        X_list += [features]\n",
    "\n",
    "print(\"extracted for /searchlights/ directory\")\n",
    "X = np.vstack(X_list)\n",
    "dir = \"/scratch/gpfs/rk1593/clustering_output/kmeans_assignments_tval_example/\"\n",
    "K = 5\n",
    "kmeans_clustering(dir = dir, X = X, K = K, searchlight_list = searchlight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ebe1f",
   "metadata": {},
   "source": [
    "# P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d664c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from scipy import stats\n",
    "import msgpack\n",
    "import orjson\n",
    "from multiprocessing import Process\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# this is to do the applied-to stuff2\n",
    "# note that matching and applied-to are synonyms in my language\n",
    "matching_id_labels_each_tr = []\n",
    "for i in range(0,17):\n",
    "    matching_id_labels_each_tr.append(0)\n",
    "for i in range(17,23):\n",
    "    matching_id_labels_each_tr.append(1)\n",
    "for i in range(23,34):\n",
    "    matching_id_labels_each_tr.append(2)\n",
    "for i in range(34,50):\n",
    "    matching_id_labels_each_tr.append(3)\n",
    "for i in range(50,66):\n",
    "    matching_id_labels_each_tr.append(4)\n",
    "for i in range(66,74):\n",
    "    matching_id_labels_each_tr.append(5)\n",
    "matching_id_labels_each_tr = np.array(matching_id_labels_each_tr)\n",
    "\n",
    "\n",
    "apply_id_to_it = {2: matching_id_labels_each_tr == 2,\n",
    "                  3: matching_id_labels_each_tr == 3,\n",
    "                  4: matching_id_labels_each_tr == 4}\n",
    "\n",
    "path_names = ['sameEv-sameSchema', 'sameEv-otherSchema', \n",
    "                            'otherEv-sameSchema', 'otherEv-otherSchema']\n",
    "\n",
    "path_abbrev_dict = {\"sEsS\": 'sameEv-sameSchema',\n",
    "                    \"sEoS\": 'sameEv-otherSchema',\n",
    "                    \"oEsS\": 'otherEv-sameSchema',\n",
    "                    \"oEoS\": 'otherEv-otherSchema'}\n",
    "\n",
    "roi_to_focus = {\"schema\": [\"2_2\", \"3_2\", \"4_2\", \"2_3\", \"3_3\", \"4_3\", \"2_4\", \"3_4\", \"4_4\"],\n",
    "                \"path\": [\"2_2\", \"3_2\", \"4_2\", \"2_3\", \"3_3\", \"4_3\", \"2_4\", \"3_4\", \"4_4\"],\n",
    "                \"rotated\": [\"2_3\", \"3_2\"],\n",
    "                \"perception\": [\"2_2\",\"3_3\",\"4_4\"]}\n",
    "\n",
    "\n",
    "\n",
    "def get_perception_value_each_tr(path_to_trs):\n",
    "    return (path_to_trs['sameEv-sameSchema'] + path_to_trs['sameEv-otherSchema']) \\\n",
    "           - (path_to_trs['otherEv-sameSchema'] + path_to_trs['otherEv-otherSchema'])\n",
    "   \n",
    "\n",
    "# 2_3 and 3_2\n",
    "def get_rotated_value_each_tr(path_to_trs, focus):\n",
    "    if focus == \"2_3\":\n",
    "        return (path_to_trs['otherEv-sameSchema'] + path_to_trs['otherEv-otherSchema']) \\\n",
    "                - (path_to_trs['sameEv-sameSchema'] + path_to_trs['sameEv-otherSchema'])\n",
    "    elif focus == \"3_2\":\n",
    "        return (path_to_trs['otherEv-sameSchema'] + path_to_trs['sameEv-otherSchema']) \\\n",
    "                - (path_to_trs['sameEv-sameSchema'] + path_to_trs['otherEv-otherSchema'])\n",
    "\n",
    "def get_path_value_each_tr(path_to_trs):\n",
    "    return path_to_trs['sameEv-sameSchema'] - path_to_trs['otherEv-sameSchema']\n",
    "\n",
    "def get_schema_value_each_tr(path_to_trs):\n",
    "    return (path_to_trs['sameEv-sameSchema'] + path_to_trs['otherEv-sameSchema']) \\\n",
    "            - (path_to_trs['sameEv-otherSchema'] + path_to_trs['otherEv-otherSchema'])\n",
    "\n",
    "# roi to event of the form x_y where x is the template event and y is the applied-to event\n",
    "def get_roi_to_focus_to_measure(pid, wedding, template_to_pid_to_cond_to_matrices, roi_in):\n",
    "    # cond for condition and path are synonyms\n",
    "    focus_to_measure = {}\n",
    "    this_measures_list = []\n",
    "    for focus in roi_to_focus[roi_in]:\n",
    "        template_id = focus[0]\n",
    "        appliedto_id = focus[2]\n",
    "        apply_it = apply_id_to_it[int(appliedto_id)]\n",
    "        path_to_trs = {}\n",
    "        for path in path_names:\n",
    "            # get the trs for this wedding, and in the correct appliedto id\n",
    "            new_trs = np.array(template_to_pid_to_cond_to_matrices[template_id][pid][path], dtype = float)[wedding,:][apply_it]\n",
    "            path_to_trs[path] = new_trs\n",
    "        if roi_in == \"schema\":\n",
    "            new_measure = get_schema_value_each_tr(path_to_trs).tolist()\n",
    "        elif roi_in == \"perception\":\n",
    "            new_measure = get_perception_value_each_tr(path_to_trs).tolist()\n",
    "        elif roi_in == \"rotated\":\n",
    "            new_measure = get_rotated_value_each_tr(path_to_trs, focus).tolist()\n",
    "        elif roi_in == \"path\":\n",
    "            new_measure = get_path_value_each_tr(path_to_trs).tolist()\n",
    "        else:\n",
    "            print(\"Error: roi invalid\")\n",
    "            return\n",
    "        focus_to_measure[focus] = new_measure\n",
    "        [this_measures_list.append(x) for x in new_measure]\n",
    "    return focus_to_measure, this_measures_list\n",
    "            \n",
    "\n",
    "def get_template_to_pid_to_cond_to_matrix_msgpack(light_id, in_dir = \"/scratch/gpfs/rk1593/clustering_output/\"):\n",
    "    template_to_pid_to_cond_to_matrices = {}\n",
    "    with open(in_dir + \"searchlights_matrices_msgpack/\" + light_id, \"rb\") as json_file:\n",
    "        template_to_pid_to_cond_to_matrices = msgpack.load(json_file, strict_map_key = False) \n",
    "    return template_to_pid_to_cond_to_matrices\n",
    "\n",
    "def get_template_to_pid_to_cond_to_matrix(light_id, in_dir = \"/scratch/gpfs/rk1593/clustering_output/\"):\n",
    "    template_to_pid_to_cond_to_matrices = {}\n",
    "    with open(in_dir + \"searchlights_matrices_orjson/\" + light_id, \"rb\") as f:\n",
    "        template_to_pid_to_cond_to_matrices = orjson.loads(f.read())\n",
    "    return template_to_pid_to_cond_to_matrices\n",
    "\n",
    "def get_roi_to_pvalue(pid_to_roi_to_measure):\n",
    "    roi_to_pvalue = {}\n",
    "    for roi in roi_to_focus.keys():\n",
    "        across_pids = []\n",
    "        for pid in pid_to_roi_to_measure:\n",
    "            across_pids.append(pid_to_roi_to_measure[pid][roi])\n",
    "        tstat,pval = stats.ttest_1samp(across_pids, popmean= 0, alternative = \"greater\")\n",
    "        roi_to_pvalue[roi] = float(pval)\n",
    "    return roi_to_pvalue\n",
    "\n",
    "def get_event_to_measure(focus_to_measure, roi):\n",
    "    event_list = [2,3,4] if roi != \"rotated\" else [2,3]\n",
    "    event_to_measure = {}\n",
    "    for event in event_list:\n",
    "        if roi == \"schema\" or roi == \"path\":\n",
    "            avg_measure = np.nanmean(np.hstack((focus_to_measure[\"2_\" + str(event)],\n",
    "                        focus_to_measure[\"3_\" + str(event)],\n",
    "                        focus_to_measure[\"4_\" + str(event)])))\n",
    "        elif roi == \"perception\":\n",
    "            avg_measure = np.nanmean(focus_to_measure[str(event) + \"_\" + str(event)])\n",
    "        elif roi == \"rotated\":\n",
    "            if event == 3:\n",
    "                avg_measure = np.nanmean(focus_to_measure[\"2_3\"])\n",
    "            elif event == 2:\n",
    "                avg_measure = np.nanmean(focus_to_measure[\"3_2\"])\n",
    "        event_to_measure[str(event)] = float(avg_measure)\n",
    "    return event_to_measure\n",
    "\n",
    "def process_chunk(light_list, output_dir):\n",
    "    for light_id in light_list:\n",
    "        template_to_pid_to_cond_to_matrices = get_template_to_pid_to_cond_to_matrix(light_id)\n",
    "        pid_to_roi_to_measure = {}\n",
    "        pid_to_roi_to_wedding_to_event_to_measure = {}\n",
    "        for pid in template_to_pid_to_cond_to_matrices[\"2\"]:\n",
    "            if pid not in pid_to_roi_to_measure:\n",
    "                pid_to_roi_to_measure[pid] = {}\n",
    "                pid_to_roi_to_wedding_to_event_to_measure[pid] = {}\n",
    "            for roi in roi_to_focus.keys():\n",
    "                if roi not in pid_to_roi_to_wedding_to_event_to_measure[pid]:\n",
    "                    pid_to_roi_to_wedding_to_event_to_measure[pid][roi] = {}\n",
    "                measures_list = []\n",
    "                for wedding in range(12):\n",
    "                    # focus\n",
    "                    focus_to_measure, new_measures = get_roi_to_focus_to_measure(pid, wedding,\n",
    "                                            template_to_pid_to_cond_to_matrices, roi)\n",
    "                    pid_to_roi_to_wedding_to_event_to_measure[pid][roi][str(wedding)] = get_event_to_measure(focus_to_measure, roi)\n",
    "                    measures_list += new_measures\n",
    "                # this takes average across weddings and tr's and focuses\n",
    "                pid_to_roi_to_measure[pid][roi] = np.nanmean(measures_list)\n",
    "        # \"for this searchlight and this measure, is the measure reliably above zero across subjects\n",
    "        roi_to_pval = get_roi_to_pvalue(pid_to_roi_to_measure)\n",
    "        # light pvals\n",
    "        if not os.path.exists(output_dir + light_id + \"_RoiToPval\"):\n",
    "            with open(output_dir + light_id + \"_RoiToPval\", \"wb\") as f:\n",
    "                f.write(orjson.dumps(roi_to_pval))\n",
    "        # pid wedding event level neural measures\n",
    "        neural_measures_path = \"/scratch/gpfs/rk1593/clustering_output/searchlights_distilled_neural_measures/\"\n",
    "        if not os.path.exists(neural_measures_path + \"each_searchlight/\" + light_id):\n",
    "            with open(neural_measures_path + \"each_searchlight/\" + light_id, \"wb\") as f:\n",
    "                f.write(orjson.dumps(pid_to_roi_to_wedding_to_event_to_measure))\n",
    "\n",
    "        # the way you can know if doing averages over chunk averages works \n",
    "        # as the same as aggregating than averaging is knowing\n",
    "        # whether or not each chunk is the same size\n",
    "\n",
    "def divide_chunks(l, chunk_size):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), chunk_size): \n",
    "        yield l[i:i + chunk_size]\n",
    "\n",
    "# event should be of the form x_y where x is the template event and y is the applied-to event\n",
    "job_id = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "output_dir = \"/scratch/gpfs/rk1593/clustering_output/brainmap_pvals_example/each_searchlight/\"\n",
    "json_path = \"/scratch/gpfs/rk1593/clustering_output/jobs_info_dict_manual_jupyter_without_tuples.json\"\n",
    "f = open(json_path,)\n",
    "jobs_info_dict = json.load(f)\n",
    "this_job_searchlights = jobs_info_dict[\"job_id_to_searchlight_subset\"][str(job_id)]\n",
    "num_chunks = 2\n",
    "chunk_size = math.floor(len(this_job_searchlights) / num_chunks)\n",
    "chunks_of_searchlights = divide_chunks(this_job_searchlights, chunk_size = chunk_size)\n",
    "processes_list = []\n",
    "\n",
    "for index,chunk in enumerate(chunks_of_searchlights):\n",
    "    new_p = Process(target = process_chunk, args = (chunk, output_dir))\n",
    "    new_p.start()\n",
    "    processes_list.append(new_p)\n",
    "\n",
    "for p in processes_list:\n",
    "    p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedding_schema [~/.conda/envs/wedding_schema/]",
   "language": "python",
   "name": "conda_wedding_schema"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
